\section{Sparse Moving Averages of Aggregated Features}
To reduce the memory consumption of SCO algorithms for GNN training, we propose to store sparse moving averages of the aggregated features. 
Instead of storing the moving averages of all nodes in the graph, we only store the moving averages of nodes that are sampled in the most recent iterations. 

\begin{figure}
  \begin{minipage}{.44\linewidth}
    \centering
  \includegraphics[scale=0.35]{figures/oom.pdf}
  \caption{Updating moving average of $\widetilde{Z}^{(l)}$.}
  \label{fig:oom}
\end{minipage} 
\hfill
\begin{minipage}{.52\linewidth}
  \centering
    \includegraphics[scale=0.35]{figures/sparse_buffer.pdf}
    \caption{Updating sparse moving average of $\widetilde{Z}^{(l)}$.}
    \label{fig:sparse_buffer}
\end{minipage}
\end{figure}

\subsection{Algorithm}
As shown in Figure~\ref{fig:sparse_buffer}, we maintain a fixed size buffer of the moving averages. 
The buffer is divided into $p$ chunks with each chunk for the $\widetilde{Z}^{(l)}$ of one iteration. 
Initially, the buffer is empty. 
In every iteration, we first look up the sampled nodes in the buffer. 
For the nodes that are found in the buffer, we collect the corresponding rows of the buffer and add them to $\widetilde{Z}_k^{(l)}$ based on Formula (\ref{eq:alg_step1}). 
For the nodes that are not found in the buffer, we multiply the corresponding rows of $\widetilde{Z}_k^{(l)}$ by $\beta_k$. 
In this example, two of the sampled nodes are found in chunk-0 and chunk-1, and the other two are not found in the buffer. 
The updated $\widetilde{Z}_k^{(l)}$ is then written to chunk-$(k\Mod{p})$.  
All the other chunks are multiplied by $(1-\beta_k)$. 
Since the sampled nodes found in the buffer are updated to chunk-$(k\Mod{p})$, the original values in chunk-0 and chunk-1 are invalidated, as shown by the shadowed rows in Figure~\ref{fig:sparse_buffer}. 


\begin{algorithm}[t]
  \small
  \caption{Updating Sparse Moving Average of Aggregated Features}
  \label{alg:update}
  \KwIn{$node\_list$, $node\_dict$, $buffer^{(l)}$, $chunk\_size$, $sampled\_nodes$, $\widetilde{Z}_k^{(l)}$, $k$, $p$}
  %\KwOut{$B=\{B_1,\ldots, B_n\}$}
  \tcp{Get the starting and ending address of chunk-($k\Mod{p}$)}
  $chunk\_start=(k\Mod{p}) * chunk\_size$\;
  $chunk\_end=chunk\_start + chunk\_size$\;
  \tcp{For each sampled node, check if it is stored in the buffer}
  \For{$i=0$ \KwTo $sampled\_nodes.length-1$}{
      $v=sampled\_nodes[i]$\;
      \tcp{If the node is in the buffer}
      \If{$v$ \textbf{\upshape in} $node\_dict$}{
        $pos = node\_dict[v]$\;
        \tcp{Update the moving average}
        $\widetilde{Z}^{(l)}_k[i]=(1-\beta_k)*buffer^{(l)}[pos]+\beta_k*\widetilde{Z}^{(l)}_k[i]$\;
        \tcp{Invalidate the original moving average of node $v$} 
        $node\_list[pos]=-1$\;
        \tcp{Overwrite the moving average of the nodes stored in chunk-($k\Mod{p}$)}
        \lIf{$node\_list[chunk\_start+i] \neq -1$}{
          $node\_dict$.remove($node\_list$[$chunk\_start+i$])
        }
        $node\_list[chunk\_start+i]=v$\;
      }
      \lElse{
        $\widetilde{Z}^{(l)}_k[i]=\beta_k*\widetilde{Z}^{(l)}_k[i]$
      }
      \tcp{The moving averages of all buffered nodes are multiplied by $(1-\beta_k)$}
      $buffer^{(l)}=(1-\beta_k)*buffer^{(l)}$\;
      \tcp{The moving averages of the sampled nodes are updated with the new values}
      $buffer^{(l)}[chunk\_start: chunk\_end]=\widetilde{Z}^{(l)}_k$\;

  }
  \end{algorithm}

Algorithm~\ref{alg:update} describes an implementation of the above procedure. 
We maintain a dictionary of the node indices with their row numbers in the buffer. 
In every iteration, we first calculate the starting and ending address of chunk-($k\Mod{p}$). 
Then, we look up every sampled node in the dictioinary. 
If the node is in the dictionary, we obtain its address in the buffer (line 6), read in its current moving average from the address, and update the corresponding row of $\widetilde{Z}_k^{(l)}$ (line 7). 
If the node is not found in the dictionary, we multiply the corresponding row of $\widetilde{Z}_k^{(l)}$ by $\beta_k$ (line 11). 
Last, we multiply the buffer by $(1-\beta_k)$ and update chunk-($k\Mod{p}$) with the new value of $\widetilde{Z}_k^{(l)}$.  
Because new values of nodes are always written to chunk-($k\Mod{p}$), there can be multiple rows in the buffer storing the moving average of the same node. 
Only the most recently updated row is valid. 
We use an array $node\_list$ to indicate the validity of the rows in the buffer. 
When we find a sampled node in the buffer, we invalidate its original moving average by setting $node\_list[pos]$ to $-1$ (line 8). 
When we overwrite a row in chunk-($k\Mod{p}$), we need to check the validity of its original value. 
If the original value is valid, we will lose the moving average of the node stored in that row, and we need to remove the node from the $node\_dict$ (line 9). 

Our algorithm uses a fixed buffer size ($chunk\_size* p$), regardless of the graph size; thus, it can be employed to train GNN on very large graphs. 

\subsection{Convergence Analysis} 
Because chunk-($k\Mod{p}$) is overwritten in iteration $k$, we lose the information of the nodes that are stored in the chunk in iteration $k-p$. 
The updating procedure in Algorithm~\ref{alg:update} can be written as 
  \begin{equation}
    \label{eq:sparse_update}
    \begin{split}
      y^{(l)}_{k+1} = &(1-\beta_{k})y^{(l)}_k + \beta_{k}  f^{(l)}_{\xi_{l,k}}(y^{(l-1)}_{k+1})\\
               &-\prod_{j=k-p+1}^{k-1}(1-\beta_j)P(\xi_{l, {k-p}}/(\xi_{l,{k-p+1}}\cup\cdots \cup\xi_{l,k}))y^{(l)}_{k-p+1}
    \end{split}
  \end{equation}
   where $P(\xi_{l, {k-p}}/(\xi_{l,{k-t+1}}\cup\cdots \cup\xi_{l,k}))$ is a projection matrix representing the nodes that are sampled in iteration $k-p$ and are not sampled in the following $p$ iterations. 
  Since these nodes are simply multiplied by $(1-\beta_j)$ in every iteration after iteration $k-p$, the values of the overwritten rows are $\prod_{j=k-p+1}^{k-1}(1-\beta_j)P(\xi_{l, {k-p}}/(\xi_{l,{k-p+1}}\cup\cdots \cup\xi_{l,k}))y^{(l)}_{k-p+1}$. 
  Our training algorithm is the same as replacing Formula (\ref{eq:alg_step1}) and (\ref{eq:alg_step2}) in the single-timescale SCO algorithm with Formula (\ref{eq:sparse_update}). 


To study the convergence rate of our training algorithm, 
we make the following assumptions that are commonly used in the analysis of SCO algorithms~\cite{yang2019multilevel, balasubramanian2020stochastic, chen2020solving}. 
\begin{assumption}
  The composite functions $f^{(l)}$ are $L_l$-smooth. That is, for any $y$ and  $y'$, we have $\| \nabla f^{(l)}_{\xi_l}(y)-\nabla f^{(l)}_{\xi_l}(y')\|\leq \|y - y'\|$. 
  \end{assumption}
 \begin{assumption}
    The stochastic gradients of the composite functions $f^{(l)}$ are bounded in expectation. That is, $\mathbb{E}[\| \nabla f^{(l)}_{\xi_l}(y)\|^2]\le C_l^2$.
 \end{assumption}
 \begin{assumption}
  The estimated aggregation results generated by the sampled neighbor aggregation is unbiased, i.e., $\mathbb{E}[f^{(l)}_{\xi_l}(y)]=f^{(l)}(y)$, and the stochastic gradient of $f^{(l)}$ is unbiased, i.e., $\mathbb{E}[\nabla f^{(l)}_{\xi_l}(y)] = \nabla f^{(l)}(y)$. 
\end{assumption}
\begin{assumption}
  The estimated aggregation results has bounded variance, i.e., $\mathbb{E}[\|\nabla f^{(l)}_{\xi_l}(y)] - \nabla f^{(l)}(y)\|\leq V^2$. 
\end{assumption}
In additional to the conventional assumptions, we make an assumption on the value of the composite functions $f^{(l)}$. 
\begin{assumption}
  The value of the composite functions $f^{(l)}$ are bounded in expectation. That is, $\mathbb{E}[\| f^{(l)}_{\xi_l}(y)\|^2]\le D_l^2$.
\end{assumption}
In the context of GNN training, the assumption means that the estimated aggregation results are bounded. 
This assumption is reasonable because the intermediate features of a GNN are always bounded. 

We have the following theorem about the convergence rate of our algorithm. 
\begin{theorem}
  Under Assumptions 1-4, if we choose $\beta_k=\beta=\frac{c_\beta}{\sqrt[3]{K}}$ and $\alpha_k=\alpha=\frac{c_\alpha}{\sqrt{K}}$, the model parameters $\{\theta_k\}$ of our training algorithm satify
\begin{equation}
  \frac{1}{K}\sum_{k=0}^{K-1}\mathbb{E}[\|\nabla F(\theta_k)\|^2] \leq \frac{2\mathcal{V}^0}{K\alpha} + \alpha B_1 + \frac{2\beta^3 B_2}{\alpha} + \frac{2(1+\beta)(1-\beta)^{2p}B_3}{\beta^3} + \frac{2(\beta+\beta^2)\beta^2V^2N}{\alpha}
\end{equation}
where $B_1=...$, and $p$ is the number of chunks used in our algorithm. 
\end{theorem}
















