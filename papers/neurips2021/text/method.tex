\section{Sparse Moving Averages of Aggregated Features}
To reduce the memory consumption of SCO algorithms for GNN training, we propose to store sparse moving averages of the aggregated features. 
Instead of storing the moving averages of all nodes in the graph, we only store the moving averages of nodes that are sampled in the most recent iterations. 

\begin{figure}
  \begin{minipage}{.44\linewidth}
    \centering
  \includegraphics[scale=0.35]{figures/oom.pdf}
  \caption{Updating moving average of $\widetilde{Z}^{(l)}$.}
  \label{fig:oom}
\end{minipage} 
\hfill
\begin{minipage}{.52\linewidth}
  \centering
    \includegraphics[scale=0.35]{figures/sparse_buffer.pdf}
    \caption{Updating sparse moving average of $\widetilde{Z}^{(l)}$.}
    \label{fig:sparse_buffer}
\end{minipage}
\end{figure}

\subsection{Algorithm}
As shown in Figure~\ref{fig:sparse_buffer}, we maintain a fixed size buffer of the moving averages. 
The buffer is divided into $p$ chunks with each chunk for the $\widetilde{Z}^{(l)}$ of one iteration. 
Initially, the buffer is empty. 
In every iteration, we first look up the sampled nodes in the buffer. 
For the nodes that are found in the buffer, we collect the corresponding rows of the buffer and add them to $\widetilde{Z}_k^{(l)}$ based on Formula (\ref{eq:alg_step1}). 
For the nodes that are not found in the buffer, we multiply the corresponding rows of $\widetilde{Z}_k^{(l)}$ by $\beta_k$. 
In this example, two of the sampled nodes are found in chunk-0 and chunk-1, and the other two are not found in the buffer. 
The updated $\widetilde{Z}_k^{(l)}$ is then written to chunk-$(k\Mod{p})$.  
All the other chunks are multiplied by $(1-\beta_k)$. 
Since the sampled nodes found in the buffer are updated to chunk-$(k\Mod{p})$, the original values in chunk-0 and chunk-1 are invalidated, as shown by the shadowed rows in Figure~\ref{fig:sparse_buffer}. 


\begin{algorithm}[t]
  \small
  \caption{Updating Sparse Moving Average of Aggregated Features}
  \label{alg:update}
  \KwIn{$node\_list$, $node\_dict$, $buffer^{(l)}$, $chunk\_size$, $sampled\_nodes$, $\widetilde{Z}_k^{(l)}$, $k$, $p$}
  %\KwOut{$B=\{B_1,\ldots, B_n\}$}
  \tcp{Get the starting and ending address of chunk-($k\Mod{p}$)}
  $chunk\_start=(k\Mod{p}) * chunk\_size$\;
  $chunk\_end=chunk\_start + chunk\_size$\;
  \tcp{For each sampled node, check if it is stored in the buffer}
  \For{$i=0$ \KwTo $sampled\_nodes.length-1$}{
      $v=sampled\_nodes[i]$\;
      \tcp{If the node is in the buffer}
      \If{$v$ \textbf{\upshape in} $node\_dict$}{
        $pos = node\_dict[v]$\;
        \tcp{Update the moving average}
        $\widetilde{Z}^{(l)}_k[i]=(1-\beta_k)*buffer^{(l)}[pos]+\beta_k*\widetilde{Z}^{(l)}_k[i]$\;
        \tcp{Invalidate the original moving average of node $v$} 
        $node\_list[pos]=-1$\;
        \tcp{Overwrite the moving average of the nodes stored in chunk-($k\Mod{p}$)}
        \lIf{$node\_list[chunk\_start+i] \neq -1$}{
          $node\_dict$.remove($node\_list$[$chunk\_start+i$])
        }
        $node\_list[chunk\_start+i]=v$\;
      }
      \lElse{
        $\widetilde{Z}^{(l)}_k[i]=\beta_k*\widetilde{Z}^{(l)}_k[i]$
      }
      \tcp{The moving averages of all buffered nodes are multiplied by $(1-\beta_k)$}
      $buffer^{(l)}=(1-\beta_k)*buffer^{(l)}$\;
      \tcp{The moving averages of the sampled nodes are updated with the new values}
      $buffer^{(l)}[chunk\_start: chunk\_end]=\widetilde{Z}^{(l)}_k$\;

  }
  \end{algorithm}

Algorithm~\ref{alg:update} describes an implementation of the above procedure. 
We maintain a dictionary of the node indices with their row numbers in the buffer. 
In every iteration, we first calculate the starting and ending address of chunk-($k\Mod{p}$). 
Then, we look up every sampled node in the dictionary. 
If the node is in the dictionary, we obtain its address in the buffer (line 6), read in its current moving average from the address, and update the corresponding row of $\widetilde{Z}_k^{(l)}$ (line 7). 
If the node is not found in the dictionary, we multiply the corresponding row of $\widetilde{Z}_k^{(l)}$ by $\beta_k$ (line 11). 
Last, we multiply the buffer by $(1-\beta_k)$ and update chunk-($k\Mod{p}$) with the new value of $\widetilde{Z}_k^{(l)}$.  
Because new values of nodes are always written to chunk-($k\Mod{p}$), there can be multiple rows in the buffer storing the moving average of the same node. 
Only the most recently updated row is valid. 
We use an array $node\_list$ to indicate the validity of the rows in the buffer. 
When we find a sampled node in the buffer, we invalidate its original moving average by setting $node\_list[pos]$ to $-1$ (line 8). 
When we overwrite a row in chunk-($k\Mod{p}$), we need to check the validity of its original value. 
If the original value is valid, we will lose the moving average of the node stored in that row, and we need to remove the node from the $node\_dict$ (line 9). 

Our algorithm uses a fixed buffer size ($chunk\_size* p$), regardless of the graph size. 
Therefore, it can be employed to train GNN on very large graphs. 

\subsection{Convergence Analysis} 
As chunk-($k\Mod{p}$) is overwritten in iteration $k$, the information of the nodes stored in the chunk in iteration $k-p$ is lost.  
The updating procedure in Algorithm~\ref{alg:update} can be written as 
  \begin{equation}
    \label{eq:sparse_update}
    \begin{split}
      y^{(l)}_{k+1} = &(1-\beta_{k})y^{(l)}_k + \beta_{k}  f^{(l)}_{\xi_{l,k}}(y^{(l-1)}_{k+1})\\
               &-\prod_{j=k-p+1}^{k-1}(1-\beta_j)P(\xi_{l, {k-p}}/(\xi_{l,{k-p+1}}\cup\cdots \cup\xi_{l,k}))y^{(l)}_{k-p+1}
    \end{split}
  \end{equation}
   where $P(\xi_{l, {k-p}}/(\xi_{l,{k-t+1}}\cup\cdots \cup\xi_{l,k}))$ is a projection matrix representing the nodes that are sampled in iteration $k-p$ and are not sampled in the following $p$ iterations. 
  Since these nodes are simply multiplied by $(1-\beta_j)$ in every iteration after iteration $k-p$, the values of the overwritten rows are $\prod_{j=k-p+1}^{k-1}(1-\beta_j)P(\xi_{l, {k-p}}/(\xi_{l,{k-p+1}}\cup\cdots \cup\xi_{l,k}))y^{(l)}_{k-p+1}$. 
  Our training algorithm is the same as replacing Formula (\ref{eq:alg_step1}) and (\ref{eq:alg_step2}) in the single-timescale SCO algorithm with Formula (\ref{eq:sparse_update}). 


To study the convergence rate of our training algorithm, 
we make the following assumptions that are commonly used in the analysis of SCO algorithms~\cite{yang2019multilevel, balasubramanian2020stochastic, chen2020solving}. 
\begin{assumption}
  The composite functions $f^{(l)}$ are $L_l$-smooth. That is, for any $y$ and  $y'$, we have $\| \nabla f^{(l)}_{\xi_l}(y)-\nabla f^{(l)}_{\xi_l}(y')\|\leq \|y - y'\|$. 
  \end{assumption}
 \begin{assumption}
    The stochastic gradients of the composite functions $f^{(l)}$ are bounded in expectation, i.e., $\mathbb{E}[\| \nabla f^{(l)}_{\xi_l}(y)\|^2]\le C_l^2$.
 \end{assumption}
 \begin{assumption}
  The estimated aggregation results obtained by sampled neighbor aggregation is unbiased, i.e., $\mathbb{E}[f^{(l)}_{\xi_l}(y)]=f^{(l)}(y)$, and the stochastic gradient of $f^{(l)}$ is unbiased, i.e., $\mathbb{E}[\nabla f^{(l)}_{\xi_l}(y)] = \nabla f^{(l)}(y)$. 
\end{assumption}
The same as the single-timescale SCO algorithm in~\cite{balasubramanian2020stochastic}, our algorithm uses large batches for estimating the composite functions, and we assume that the estimation variance is small enough.  
\begin{assumption}
  The estimated aggregation results have small bounded variance, i.e., $\mathbb{E}[\|\nabla f^{(l)}_{\xi_l, k}(y)] - \nabla f^{(l)}(y)\|\leq \beta_k V^2$. 
\end{assumption}
This is a reasonable assumption for GNN training on GPUs as we always sample a batch of nodes for neighbor aggregation to achieve better utilization of the GPU parallelism. 

In additional to the conventional assumptions, we make an assumption on the value of the composite functions. 
\begin{assumption}
  The estimated aggregation results are bounded, i.e., $\mathbb{E}[\| f^{(l)}_{\xi_l}(y)\|^2]\le D_l^2$.
\end{assumption}

The convergence rate of our algorithm is summarized in the following theorem. 
\begin{theorem}
  Under Assumptions 1-5, if we choose $\alpha_k=\alpha=\frac{c_\alpha}{\sqrt{K}}$ and $\beta_k=\beta={\alpha}(1+\sum_{l=1}^{N-1}A_l^2)/2=\frac{c_{\beta}}{\sqrt{K}}$, the model parameters $\{\theta_k\}$ of our training algorithm with (\ref{eq:sparse_update}) for updating sparse moving averages satify
\begin{equation}
  \label{eq:th1}
  \begin{split}
  \frac{1}{K}\sum_{k=0}^{K-1}\mathbb{E}[\|\nabla F(\theta_k)\|^2] \leq&  \frac{2\mathcal{V}_0}{K\alpha} + \alpha B_1 + \frac{2\beta^2 B_2}{\alpha}+ \frac{4\beta^3V^2N}{\alpha} + \frac{2(1+\beta)(1-\beta)^{2p}B_3}{\alpha\beta^2} \\
= &\frac{2\mathcal{V}_0c_{\alpha}+c_{\alpha} B_1 + 2(c_{\beta}^2/c_\alpha) B_2}{\sqrt{K}} + \frac{4(c_{\beta}^3/c_\alpha) V^2N}{K} + \frac{2(1-\beta)^{2p-1}B_3}{\alpha\beta^2}
  \end{split}
\end{equation}
where $B_1=...$, and $p$ is the number of chunks used in our algorithm. 
\end{theorem}
The last term on the RHS of (\ref{eq:th1}) shows how the convergence of the SCO algorithm is affected by the sparse moving averages. 
The larger $p$ we use (i.e., the more chunks we have in the buffer), the smaller $(1-\beta)^{2p}$ we have, and the faster convergence we achieve. 
In theory, if we can make $(1-\beta)^{2p-1}=O(\beta^{4})$, the algorithm will achieve $O(\sqrt{1/K})$ convergence rate. 
As $\beta\rightarrow 0$, we need larger $p$ to maintain the convergence rate, and eventually, we will need to store the moving average of all nodes in the graph to the buffer.  
In practice, we usually set $\beta$ to a fixed value, and we can use a fixed $p$ such that $(1-\beta)^{2p-1}$ is small enough. 

Our sparse moving average can also be applied to other SCO algorithms. For example, the Stochastically Corrected Stochastic Compositional gradient method (SCSC)~\cite{chen2020solving} has a correction term in the update of the moving averages. 
Because of the correction term, SCSC needs a relaxed assumption on the estimation error of the composite functions. 
More specifically, if we change (\ref{eq:sparse_update}) to 
\begin{equation}
  \label{eq:sparse_update2}
  \begin{split}
    y^{(l)}_{k+1} = &(1-\beta_{k})y^{(l)}_k + f^{(l)}_{\xi_{l,k}}(y^{(l-1)}_{k+1}) - (1-\beta_k)f^{(l)}_{\xi_{l,k}}(y^{(l-1)}_{k})\\
             &-\prod_{j=k-p+1}^{k-1}(1-\beta_j)P(\xi_{l, {k-p}}/(\xi_{l,{k-p+1}}\cup\cdots \cup\xi_{l,k}))y^{(l)}_{k-p+1}
  \end{split}
\end{equation}
and replace (\ref{eq:alg_step1}) and (\ref{eq:alg_step2}) with (\ref{eq:sparse_update2}), we can replace Assumption 4 with
\begin{assumption}
  The estimated aggregation results have bounded variance, i.e., $\mathbb{E}[\|\nabla f^{(l)}_{\xi_l, k}(y)] - \nabla f^{(l)}(y)\|\leq V^2$. 
\end{assumption}
The convergence rate of SCSC with sparse moving averages is summarized as follows. 
\begin{theorem}
  Under Assumptions 1-3 and 5-6, if we choose $\alpha_k=\alpha=\frac{c_\alpha}{\sqrt{K}}$ and $\beta_k=\beta={\alpha}\sum_{l=1}^{N-1}A_l^2=\frac{c_{\beta}}{\sqrt{K}}$, the model parameters $\{\theta_k\}$ of SCSC with (\ref{eq:sparse_update2}) for updating sparse moving averages satify
\begin{equation}
  \label{eq:th2}
  \begin{split}
  \frac{1}{K}\sum_{k=0}^{K-1}\mathbb{E}[\|\nabla F(\theta_k)\|^2] \leq&  \frac{2\mathcal{V}_0}{K\alpha} + \alpha B_1 + \frac{\beta^2 B_2}{\alpha} + \frac{(1+\beta)(1-\beta)^{2p}B_3}{\alpha\beta} + \frac{(1-\beta)^{2p-2}B_4}{\alpha}\\
= &\frac{2\mathcal{V}_0c_{\alpha}+c_{\alpha} B_1 + (c_{\beta}^2/c_\alpha)B_2}{\sqrt{K}}  + \frac{(1-\beta)^{2p-1}B_3}{\alpha\beta} + \frac{(1-\beta)^{2p-2}B_4}{\alpha}
  \end{split}
\end{equation}
where $B_1=...$, and $p$ is the number of chunks used in our algorithm. 
\end{theorem}
The results indicate that, if $(1-\beta)^{2p-1}=O(\beta^3)$ and $(1-\beta)^{2p-2}=O(\beta^2)$, the algorithm achieves $O(\sqrt{1/K})$ convergence rate. 


















