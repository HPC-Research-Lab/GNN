\section{Introduction}
Graph Neural Networks (GNNs) have become the state-of-the-art models for machine learning tasks on graph-structured data~\cite{kipf2017semi, duran2017learning, zhang2017weisfeiler, zhang2018link, ying2018hierarchical, gilmer2017neural}. 
They have been applied to many application domains, including content recommendation~\cite{ying2018graph}, traffic prediction~\cite{zhang2018gaan},  drug discovery~\cite{li2018learning}, and molecular property prediction~\cite{gilmer2017neural}.  

Despite the success of GNNs, it is hard to deploy GNNs to applications that involve large-scale graphs. 
To train a GNN on a graph, data need to be recursively aggregated from all neighboring nodes in every iteration. 
This recursive aggregation incurs a large computation and data movement overhead. 
To alleviate the computation burden, various neighbor sampling methods have been proposed~\cite{hamilton2017inductive, ying2018graph, chen2018fastgcn, zou2019layer, AAAI1816642, chiang2019cluster, Zeng2020GraphSAINT}. 
The idea is to compute an unbiased estimate of the aggregation result based on a sampled subset of the neighbors. 
These sampling techniques enable GNN training on large graphs. 
However, 
due to the composition of the aggregation functions in multiple layers, 
the stochastic gradient obtained with sampled neighbor aggregation is not an unbiased estimate of the true gradient, which undermines the convergence property of SGD-based training algorithms. 

Previous work has shown that sampling-based GNN training is actually a Stochastic Compositional Optimization (SCO) problem~\cite{cong2020minimal,  cong2021importance}. 
Cong {\em et al.}~\cite{cong2021importance} show that SCO algorithms can achieve faster convergence than the commonly used Adam SGD for GNN training on small graphs (with less than one million nodes). 
However, these SCO algorithms are impractical for training GNN on large graphs due to the large memory consumption. 
Despite the different forms of SCO algorithms, they all need to maintain the moving averages of the estimation of the composite functions~\cite{yang2019multilevel, balasubramanian2020stochastic, chen2020solving, lian2017finite, wang2017accelerating, ghadimi2020single}. 
For GNN training, they need to store the moving averages of the aggregation results of all nodes in the graph. 
The larger the graphs, the more space these moving averages take. 
Since the GPU has limited memory, these SCO algorithms can easily run out of memory for large-scale GNN training, even with aggressive neighbor sampling. 
While it is possible to store the moving averages in CPU memory, copying the data from CPU to GPU in each iteration is expensive, which may negate the benefits of the faster convergence of SCO algorithms. 

In this work, we target the large memory consumption issue of SCO algorithms and propose a new algorithm that maintains sparse moving averages of the aggregated features for GNN training. 
Instead of storing the moving averages of the aggregation results of all nodes, our algorithm only stores the aggregated features of nodes sampled in the most recent iterations. 
We show that when the number of iterations satisfies certain constraints the convergence rate of the original SCO algorithms can be preserved. 
We also exploit the skewness of sampling probability distribution in real-world graphs and propose a more efficient buffering strategy that further reduces the moving average size. 
Our experiments with two different GNN models on different graphs validate our theoretical results and show that our algorithm achieves faster convergence than Adam SGD for GNN training with small memory consumption. 




















